"""
This type stub file was generated by pyright.
"""

import sqlite3
import os
import sys
import tempfile
import threading
import logging
import traceback
import weakref
from base64 import b64decode, b64encode
from cPickle import HIGHEST_PROTOCOL as PICKLE_PROTOCOL, dumps, loads
from collections import UserDict as DictClass
from queue import Queue

"""
A lightweight wrapper around Python's sqlite3 database, with a dict-like interface
and multi-thread access support::

>>> mydict = SqliteDict('some.db', autocommit=True) # the mapping will be persisted to file `some.db`
>>> mydict['some_key'] = any_picklable_object
>>> print mydict['some_key']
>>> print len(mydict) # etc... all dict functions work

Pickle is used internally to serialize the values. Keys are strings.

If you don't use autocommit (default is no autocommit for performance), then
don't forget to call `mydict.commit()` when done with a transaction.

"""
__version__ = ...
def reraise(tp, value, tb=...):
    ...

logger = ...
_REQUEST_CLOSE = ...
_REQUEST_COMMIT = ...
_RESPONSE_NO_MORE = ...
def open(*args, **kwargs): # -> SqliteDict:
    """See documentation of the SqliteDict class."""
    ...

def encode(obj): # -> Binary:
    """Serialize an object using pickle to a binary format accepted by SQLite."""
    ...

def decode(obj): # -> Any:
    """Deserialize objects retrieved from SQLite."""
    ...

def encode_key(key): # -> str:
    """Serialize a key using pickle + base64 encoding to text accepted by SQLite."""
    ...

def decode_key(key): # -> Any:
    """Deserialize a key retrieved from SQLite."""
    ...

def identity(obj):
    """Identity f(x) = x function for encoding/decoding."""
    ...

class SqliteDict(DictClass):
    VALID_FLAGS = ...
    def __init__(self, filename=..., tablename=..., flag=..., autocommit=..., journal_mode=..., encode=..., decode=..., encode_key=..., decode_key=..., timeout=..., outer_stack=...) -> None:
        """
        Initialize a thread-safe sqlite-backed dictionary. The dictionary will
        be a table `tablename` in database file `filename`. A single file (=database)
        may contain multiple tables.

        If no `filename` is given, a random file in temp will be used (and deleted
        from temp once the dict is closed/deleted).

        If you enable `autocommit`, changes will be committed after each operation
        (more inefficient but safer). Otherwise, changes are committed on `self.commit()`,
        `self.clear()` and `self.close()`.

        Set `journal_mode` to 'OFF' if you're experiencing sqlite I/O problems
        or if you need performance and don't care about crash-consistency.

        Set `outer_stack` to False to disable the output of the outer exception
        to the error logs.  This may improve the efficiency of sqlitedict
        operation at the expense of a detailed exception trace.

        The `flag` parameter. Exactly one of:
          'c': default mode, open for read/write, creating the db/table if necessary.
          'w': open for r/w, but drop `tablename` contents first (start with empty table)
          'r': open as read-only
          'n': create a new database (erasing any existing tables, not just `tablename`!).

        The `encode` and `decode` parameters are used to customize how the values
        are serialized and deserialized.
        The `encode` parameter must be a function that takes a single Python
        object and returns a serialized representation.
        The `decode` function must be a function that takes the serialized
        representation produced by `encode` and returns a deserialized Python
        object.
        The default is to use pickle.

        The `timeout` defines the maximum time (in seconds) to wait for initial Thread startup.

        """
        ...
    
    def __enter__(self): # -> Self@SqliteDict:
        ...
    
    def __exit__(self, *exc_info): # -> None:
        ...
    
    def __str__(self) -> str:
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __len__(self): # -> Literal[0]:
        ...
    
    def __bool__(self): # -> bool:
        ...
    
    def iterkeys(self): # -> Generator[Unknown, Any, None]:
        ...
    
    def itervalues(self): # -> Generator[Unknown | Any, Any, None]:
        ...
    
    def iteritems(self): # -> Generator[tuple[Unknown, Unknown | Any], Any, None]:
        ...
    
    def keys(self): # -> Generator[Unknown, Any, None]:
        ...
    
    def values(self): # -> Generator[Unknown | Any, Any, None]:
        ...
    
    def items(self): # -> Generator[tuple[Unknown, Unknown | Any], Any, None]:
        ...
    
    def __contains__(self, key): # -> bool:
        ...
    
    def __getitem__(self, key): # -> Any:
        ...
    
    def __setitem__(self, key, value): # -> None:
        ...
    
    def __delitem__(self, key): # -> None:
        ...
    
    def update(self, items=..., **kwds): # -> None:
        ...
    
    def __iter__(self): # -> Generator[Unknown, Any, None]:
        ...
    
    def clear(self): # -> None:
        ...
    
    @staticmethod
    def get_tablenames(filename): # -> list[Any]:
        """get the names of the tables in an sqlite db as a list"""
        ...
    
    def commit(self, blocking=...): # -> None:
        """
        Persist all data to disk.

        When `blocking` is False, the commit command is queued, but the data is
        not guaranteed persisted (default implication when autocommit=True).
        """
        ...
    
    sync = ...
    def close(self, do_log=..., force=...): # -> None:
        ...
    
    def terminate(self): # -> None:
        """Delete the underlying database file. Use with care."""
        ...
    
    def __del__(self): # -> None:
        ...
    


class SqliteMultithread(threading.Thread):
    """
    Wrap sqlite connection in a way that allows concurrent requests from multiple threads.

    This is done by internally queueing the requests and processing them sequentially
    in a separate thread (in the same order they arrived).

    """
    def __init__(self, filename, autocommit, journal_mode, outer_stack=...) -> None:
        ...
    
    def run(self): # -> None:
        ...
    
    def check_raise_error(self): # -> None:
        """
        Check for and raise exception for any previous sqlite query.

        For the `execute*` family of method calls, such calls are non-blocking and any
        exception raised in the thread cannot be handled by the calling Thread (usually
        MainThread).  This method is called on `close`, and prior to any subsequent
        calls to the `execute*` methods to check for and raise an exception in a
        previous call to the MainThread.
        """
        ...
    
    def execute(self, req, arg=..., res=...): # -> None:
        """
        `execute` calls are non-blocking: just queue up the request and return immediately.

        :param req: The request (an SQL command)
        :param arg: Arguments to the SQL command
        :param res: A queue in which to place responses as they become available
        """
        ...
    
    def executemany(self, req, items): # -> None:
        ...
    
    def select(self, req, arg=...): # -> Generator[Unknown, Any, None]:
        """
        Unlike sqlite's native select, this select doesn't handle iteration efficiently.

        The result of `select` starts filling up with values as soon as the
        request is dequeued, and although you can iterate over the result normally
        (`for res in self.select(): ...`), the entire result will be in memory.
        """
        ...
    
    def select_one(self, req, arg=...): # -> None:
        """Return only the first row of the SELECT, or None if there are no matching rows."""
        ...
    
    def commit(self, blocking=...): # -> None:
        ...
    
    def close(self, force=...): # -> None:
        ...
    


if __name__ == '__main__':
    ...
